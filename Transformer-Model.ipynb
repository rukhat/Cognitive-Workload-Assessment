{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Himon Thakur\"\n",
    "__credits__ = [\"Himon Thakur\"]\n",
    "__license__ = \"Apache 2.0\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Himon Thakur\"\n",
    "__email__ = \"hthakur@uccs.edu\"\n",
    "__status__ = \"Prototype\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mne.io import read_raw_eeglab\n",
    "from mne.preprocessing import ICA\n",
    "from autoreject import AutoReject\n",
    "import os\n",
    "from mne.decoding import CSP\n",
    "from scipy.fft import fft\n",
    "import matplotlib.pyplot as plt\n",
    "from mne.decoding import Scaler\n",
    "\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "root_dir = 'dataset'\n",
    "output_dir = 'processed_data'\n",
    "subjects = [f\"{i:02d}\" for i in range(1, 30)]  # List of subjects to process\n",
    "sessions = ['S1', 'S2', 'S3']  \n",
    "montage = 'standard_1005'  \n",
    "task_type = 'nback'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data(subject, session, task_files=None):\n",
    "    sub_dir = os.path.join(root_dir, f'sub-{subject}', f'ses-{session}', 'eeg')\n",
    "    raw_list = []\n",
    "    \n",
    "    if task_files is None:\n",
    "        if task_type == 'nback':\n",
    "            task_files = ['zeroBACK', 'oneBACK', 'twoBACK']\n",
    "        elif task_type == 'matb':\n",
    "            task_files = ['MATBdiff']\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown task type: {task_type}\")\n",
    "    \n",
    "    print(f\"Loading data for subject {subject}, session {session}\")\n",
    "    \n",
    "    for task_file in task_files:\n",
    "        fpath = os.path.join(sub_dir, f\"{task_file}.set\")\n",
    "        if not os.path.exists(fpath):\n",
    "            print(f\"Missing file: {fpath}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            print(f\"Loading {task_file}.set\")\n",
    "            raw = read_raw_eeglab(fpath, preload=True)\n",
    "            raw.set_montage(montage)\n",
    "            \n",
    "            raw.annotations.append(\n",
    "                onset=0,\n",
    "                duration=raw.times[-1],\n",
    "                description=f\"FILE_{task_file.upper()}\"\n",
    "            )\n",
    "            \n",
    "            raw_list.append(raw)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {task_file}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not raw_list:\n",
    "        raise FileNotFoundError(f\"No valid EEG files found for sub-{subject}, ses-{session}\")\n",
    "    \n",
    "    print(f\"Concatenating {len(raw_list)} files\")\n",
    "    raw_concat = mne.concatenate_raws(raw_list)\n",
    "    \n",
    "    return raw_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_raw_data(raw):\n",
    "    print(\"Preprocessing raw data\")\n",
    "    \n",
    "    raw_clean = raw.copy()\n",
    "    \n",
    "    # Apply filters\n",
    "    print(\"Applying filters\")\n",
    "    raw_clean.filter(1.0, 40.0, fir_design='firwin')\n",
    "    raw_clean.notch_filter(np.arange(50, 125, 50))\n",
    "    \n",
    "    # Apply ICA for artifact removal\n",
    "    print(\"Applying ICA for artifact removal\")\n",
    "    ica = ICA(n_components=20, random_state=97)\n",
    "    ica.fit(raw_clean)\n",
    "    \n",
    "    # Find components related to eye movements/blinks\n",
    "    eog_indices = []\n",
    "    \n",
    "    # Look for EOG channels\n",
    "    eog_channels = [ch for ch in raw_clean.ch_names if 'EOG' in ch.upper()]\n",
    "    \n",
    "    if eog_channels:\n",
    "        print(f\"Using EOG channels: {eog_channels}\")\n",
    "        for ch_name in eog_channels:\n",
    "            indices, scores = ica.find_bads_eog(raw_clean, ch_name=ch_name)\n",
    "            eog_indices.extend(indices)\n",
    "    else:\n",
    "        frontals = ['Fp1', 'Fp2', 'F7', 'F8']\n",
    "        frontal_chs = [ch for ch in frontals if ch in raw_clean.ch_names]\n",
    "        \n",
    "        if frontal_chs:\n",
    "            print(f\"No EOG channels found. Using frontal channels: {frontal_chs}\")\n",
    "            for ch in frontal_chs:\n",
    "                eog_indices.extend(ica.find_bads_eog(raw_clean, ch_name=ch)[0])\n",
    "    \n",
    "    if not eog_indices and ica.n_components_ > 0:\n",
    "        eog_indices = [0]\n",
    "        print(\"No EOG components found automatically. Excluding first component.\")\n",
    "    \n",
    "    eog_indices = list(set(eog_indices))\n",
    "    \n",
    "    print(f\"Excluding ICA components: {eog_indices}\")\n",
    "    ica.exclude = eog_indices\n",
    "    ica.apply(raw_clean)\n",
    "    \n",
    "    print(\"Setting average reference\")\n",
    "    raw_clean.set_eeg_reference('average')\n",
    "    \n",
    "    return raw_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_epochs(raw, tmin=-0.2, tmax=1.0, baseline=(None, 0)):\n",
    "    print(\"Creating epochs\")\n",
    "    \n",
    "    # Extract events from annotations\n",
    "    events, event_id = mne.events_from_annotations(raw)\n",
    "    print(f\"Found event types: {event_id}\")\n",
    "    \n",
    "    if task_type == 'nback':\n",
    "        nback_files = [desc for desc in raw.annotations.description if 'FILE_' in desc]\n",
    "        print(f\"Found n-back files: {nback_files}\")\n",
    "        \n",
    "        task_event_id = {}\n",
    "        \n",
    "        for event_name, event_code in event_id.items():\n",
    "            event_lower = str(event_name).lower()\n",
    "            \n",
    "            # Look for trial markers\n",
    "            if ('zeroback' in event_lower or '0back' in event_lower) and ('trial' in event_lower or 'onset' in event_lower):\n",
    "                task_event_id['0back'] = event_code\n",
    "            elif ('oneback' in event_lower or '1back' in event_lower) and ('trial' in event_lower or 'onset' in event_lower):\n",
    "                task_event_id['1back'] = event_code\n",
    "            elif ('twoback' in event_lower or '2back' in event_lower) and ('trial' in event_lower or 'onset' in event_lower):\n",
    "                task_event_id['2back'] = event_code\n",
    "            \n",
    "            # Check for more specific trial markers based on the triggerlist.txt\n",
    "            if event_name in ['6021', '6022', '6023']:  # ZEROBACK Trial markers\n",
    "                task_event_id['0back'] = event_code\n",
    "            elif event_name in ['6121', '6122', '6123']:  # ONEBACK Trial markers\n",
    "                task_event_id['1back'] = event_code\n",
    "            elif event_name in ['6221', '6222', '6223']:  # TWOBACK Trial markers\n",
    "                task_event_id['2back'] = event_code\n",
    "        \n",
    "        if not task_event_id:\n",
    "            print(\"No specific task events found. Using trial onset markers.\")\n",
    "            for event_name, event_code in event_id.items():\n",
    "                try:\n",
    "                    event_str = str(event_name)\n",
    "                    \n",
    "                    if event_str.isdigit():\n",
    "                        code = int(event_str)\n",
    "                        # ZEROBACK Trial onset markers\n",
    "                        if code in [6021, 6022, 6023]:\n",
    "                            task_event_id['0back'] = event_code\n",
    "                        # ONEBACK Trial onset markers\n",
    "                        elif code in [6121, 6122, 6123]:\n",
    "                            task_event_id['1back'] = event_code\n",
    "                        # TWOBACK Trial onset markers\n",
    "                        elif code in [6221, 6222, 6223]:\n",
    "                            task_event_id['2back'] = event_code\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        if not task_event_id:\n",
    "            print(\"Still no task events found. Filtering out non-trial events.\")\n",
    "            task_event_id = {name: code for name, code in event_id.items() \n",
    "                            if str(name) not in ['boundary', 'FILE_ZEROBACK', 'FILE_ONEBACK', 'FILE_TWOBACK']}\n",
    "        \n",
    "        print(f\"Using event dictionary: {task_event_id}\")\n",
    "    else:\n",
    "        task_event_id = {name: code for name, code in event_id.items() \n",
    "                        if 'boundary' not in str(name) and 'FILE_' not in str(name)}\n",
    "    \n",
    "    # Create epochs - handle repeated events\n",
    "    epochs = mne.Epochs(\n",
    "        raw, events, event_id=task_event_id,\n",
    "        tmin=tmin, tmax=tmax, baseline=baseline,\n",
    "        preload=True, on_missing='warn',\n",
    "        event_repeated='drop'  # Handle repeated events by dropping them\n",
    "    )\n",
    "    \n",
    "    print(f\"Created {len(epochs)} epochs\")\n",
    "    \n",
    "    # Apply AutoReject to clean epochs\n",
    "    print(\"Applying AutoReject for automatic artifact rejection\")\n",
    "    ar = AutoReject(n_interpolate=[1, 2, 4], random_state=42)\n",
    "    epochs_clean, reject_log = ar.fit_transform(epochs, return_log=True)\n",
    "    \n",
    "    print(f\"Kept {len(epochs_clean)}/{len(epochs)} epochs after rejection\")\n",
    "    \n",
    "    return epochs_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(epochs):\n",
    "    print(\"Extracting features\")\n",
    "    \n",
    "    # Initialize feature dictionary\n",
    "    features = {\n",
    "        'label': epochs.events[:, -1],\n",
    "        'csp': np.full((len(epochs), 4), np.nan),\n",
    "        'psd': [],\n",
    "        'fft': []\n",
    "    }\n",
    "\n",
    "    # 1. CSP Features\n",
    "    print(\"Extracting CSP features\")\n",
    "    unique_labels = np.unique(features['label'])\n",
    "    \n",
    "    if len(unique_labels) >= 2:\n",
    "        try:\n",
    "            epochs_csp = epochs.copy().pick_types(eeg=True)\n",
    "            \n",
    "            sel = np.concatenate([\n",
    "                np.where(epochs_csp.events[:, -1] == unique_labels[0])[0],\n",
    "                np.where(epochs_csp.events[:, -1] == unique_labels[-1])[0]\n",
    "            ])\n",
    "            \n",
    "            valid_sel = [idx for idx in sel if idx < len(epochs_csp)]\n",
    "            \n",
    "            if len(valid_sel) > 5:\n",
    "                # Apply CSP\n",
    "                csp = CSP(n_components=4, reg=None, log=True)\n",
    "                X_csp = csp.fit_transform(epochs_csp[valid_sel].get_data(), \n",
    "                                         epochs_csp[valid_sel].events[:, -1])\n",
    "                \n",
    "                # Store CSP features\n",
    "                for i, idx in enumerate(valid_sel):\n",
    "                    features['csp'][idx] = X_csp[i]\n",
    "                print(f\"Extracted CSP features from {len(valid_sel)} epochs\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing CSP features: {str(e)}\")\n",
    "    \n",
    "    # 2. PSD Features\n",
    "    print(\"Extracting PSD features\")\n",
    "    # Define frequency bands (Hz)\n",
    "    freq_bands = [(4, 8), (8, 13), (13, 30), (30, 40)]  # theta, alpha, beta, gamma\n",
    "    \n",
    "    # Get all epochs data\n",
    "    X = epochs.get_data()  # shape: (n_epochs, n_channels, n_times)\n",
    "    \n",
    "    # Process each epoch\n",
    "    for epoch_idx in range(X.shape[0]):\n",
    "        print(f\"Processing epoch {epoch_idx+1}/{X.shape[0]}\", end='\\r')\n",
    "        epoch = X[epoch_idx]\n",
    "        \n",
    "        # Manual Z-score normalization for each channel\n",
    "        epoch_norm = np.zeros_like(epoch)\n",
    "        for ch_idx in range(epoch.shape[0]):\n",
    "            ch_data = epoch[ch_idx]\n",
    "            ch_mean = np.mean(ch_data)\n",
    "            ch_std = np.std(ch_data)\n",
    "            # Avoid division by zero\n",
    "            if ch_std > 0:\n",
    "                epoch_norm[ch_idx] = (ch_data - ch_mean) / ch_std\n",
    "            else:\n",
    "                epoch_norm[ch_idx] = ch_data - ch_mean\n",
    "        \n",
    "        # Calculate PSD using Welch's method\n",
    "        psd, freqs = mne.time_frequency.psd_array_welch(\n",
    "            epoch_norm, sfreq=epochs.info['sfreq'],\n",
    "            fmin=1, fmax=40, n_fft=256, n_overlap=128\n",
    "        )\n",
    "        \n",
    "        # Calculate band power for each frequency band\n",
    "        band_powers = []\n",
    "        for fmin, fmax in freq_bands:\n",
    "            # Find frequencies within the band\n",
    "            freq_mask = (freqs >= fmin) & (freqs <= fmax)\n",
    "            \n",
    "            band_power = np.log10(psd[:, freq_mask].mean(axis=1) + 1e-10)\n",
    "            band_powers.append(band_power)\n",
    "        \n",
    "        features['psd'].append(band_powers)\n",
    "\n",
    "    # 3. FFT Features\n",
    "    print(\"\\nExtracting FFT features\")\n",
    "    for epoch_idx, epoch in enumerate(X):\n",
    "        print(f\"Processing epoch {epoch_idx+1}/{X.shape[0]}\", end='\\r')\n",
    "        # Apply Hanning window\n",
    "        window = np.hanning(epoch.shape[1])\n",
    "        fft_vals = np.abs(fft(epoch * window[np.newaxis, :], axis=1))\n",
    "        \n",
    "        # Calculate frequency axis\n",
    "        freqs = np.fft.fftfreq(epoch.shape[1], 1/epochs.info['sfreq'])\n",
    "        mask = (freqs >= 0) & (freqs <= 40)  # Keep only 0-40 Hz\n",
    "        \n",
    "        # For each channel, extract: peak frequency, peak amplitude, mean amplitude\n",
    "        channel_features = []\n",
    "        for ch_idx in range(epoch.shape[0]):\n",
    "            ch_fft = fft_vals[ch_idx, mask]\n",
    "            ch_freqs = freqs[mask]\n",
    "            \n",
    "            # Find peak frequency\n",
    "            peak_idx = np.argmax(ch_fft)\n",
    "            peak_freq = ch_freqs[peak_idx]\n",
    "            peak_amp = ch_fft[peak_idx]\n",
    "            mean_amp = np.mean(ch_fft)\n",
    "            \n",
    "            channel_features.append([peak_freq, peak_amp, mean_amp])\n",
    "        \n",
    "        features['fft'].append(channel_features)\n",
    "    \n",
    "    print(\"\\nConverting features to DataFrame\")\n",
    "    \n",
    "    # Convert features to a DataFrame\n",
    "    feature_dict = {\n",
    "        'label': features['label']\n",
    "    }\n",
    "    \n",
    "    # Add CSP features\n",
    "    for i in range(4):\n",
    "        feature_dict[f'csp_{i}'] = features['csp'][:, i]\n",
    "    \n",
    "    # Add PSD features - one column for each channel and frequency band\n",
    "    psd_array = np.array(features['psd'])\n",
    "    for band_idx, (fmin, fmax) in enumerate(freq_bands):\n",
    "        for ch in range(psd_array.shape[2]):  # For each channel\n",
    "            feature_dict[f'psd_{fmin}_{fmax}_ch{ch}'] = psd_array[:, band_idx, ch]\n",
    "    \n",
    "    # Add FFT features - one column for each channel and feature type\n",
    "    fft_array = np.array(features['fft'])\n",
    "    for ch in range(fft_array.shape[1]):  # For each channel\n",
    "        feature_dict[f'fft_peak_freq_ch{ch}'] = fft_array[:, ch, 0]\n",
    "        feature_dict[f'fft_peak_amp_ch{ch}'] = fft_array[:, ch, 1]\n",
    "        feature_dict[f'fft_mean_amp_ch{ch}'] = fft_array[:, ch, 2]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(feature_dict)\n",
    "    \n",
    "    # Map event codes to class labels\n",
    "    label_mapping = {}\n",
    "    for name, code in epochs.event_id.items():\n",
    "        label_mapping[code] = name\n",
    "    \n",
    "    df['class'] = df['label'].map(label_mapping)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_features(df):\n",
    "    print(\"Validating features\")\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    \n",
    "    # Check for NaN values in numeric columns\n",
    "    nan_cols = [col for col in numeric_cols if df[col].isna().any()]\n",
    "    if nan_cols:\n",
    "        print(f\"Found NaN values in columns: {nan_cols}\")\n",
    "        \n",
    "        # For CSP features, fill with zeros\n",
    "        csp_cols = [col for col in nan_cols if 'csp' in col]\n",
    "        if csp_cols:\n",
    "            print(f\"Filling NaN values in CSP columns with zeros\")\n",
    "            df[csp_cols] = df[csp_cols].fillna(0)\n",
    "        \n",
    "        # For other features, use column median\n",
    "        other_cols = [col for col in nan_cols if col not in csp_cols]\n",
    "        if other_cols:\n",
    "            print(f\"Filling NaN values in other columns with column median\")\n",
    "            for col in other_cols:\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    # Check for infinite values in numeric columns\n",
    "    for col in numeric_cols:\n",
    "        # Check for inf values\n",
    "        mask = np.isinf(df[col].values)\n",
    "        if mask.any():\n",
    "            print(f\"Found infinite values in column: {col}\")\n",
    "            # Get finite values to calculate replacement values\n",
    "            finite_values = df[col][~mask]\n",
    "            \n",
    "            if len(finite_values) > 0:\n",
    "                max_val = finite_values.max()\n",
    "                min_val = finite_values.min()\n",
    "                \n",
    "                # Replace inf with max and -inf with min\n",
    "                df.loc[df[col] == np.inf, col] = max_val\n",
    "                df.loc[df[col] == -np.inf, col] = min_val\n",
    "            else:\n",
    "                # If no finite values, replace with 0\n",
    "                df[col] = df[col].replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    # Check class balance\n",
    "    class_counts = df['class'].value_counts()\n",
    "    print(f\"Class distribution:\\n{class_counts}\")\n",
    "    \n",
    "    if len(class_counts) < 2:\n",
    "        print(\"Warning: Only one class found in the data\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_distributions(df, output_path):\n",
    "    print(f\"Plotting feature distributions to {output_path}\")\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Plot PSD features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    psd_cols = [col for col in df.columns if 'psd' in col][:10]  # First 10 PSD features\n",
    "    \n",
    "    for i, col in enumerate(psd_cols):\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        for class_name in df['class'].unique():\n",
    "            plt.hist(df[df['class'] == class_name][col], bins=20, alpha=0.5, label=class_name)\n",
    "        plt.title(col, fontsize=8)\n",
    "        plt.legend(fontsize=6)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subject_session(subject, session):\n",
    "    print(f\"\\n=== Processing subject {subject}, session {session} ===\")\n",
    "    \n",
    "    # Create output directories\n",
    "    subject_output_dir = os.path.join(output_dir, f\"sub-{subject}\", f\"ses-{session}\")\n",
    "    os.makedirs(subject_output_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # 1. Load raw data\n",
    "        raw = load_raw_data(subject, session)\n",
    "        \n",
    "        # 2. Preprocess\n",
    "        raw_clean = preprocess_raw_data(raw)\n",
    "        \n",
    "        # Optionally save preprocessed raw data\n",
    "        raw_clean_file = os.path.join(subject_output_dir, \"raw_clean.fif\")\n",
    "        raw_clean.save(raw_clean_file, overwrite=True)\n",
    "        print(f\"Saved preprocessed raw data to {raw_clean_file}\")\n",
    "        \n",
    "        # 3. Create epochs\n",
    "        epochs = create_epochs(raw_clean)\n",
    "        \n",
    "        # Save epochs\n",
    "        epochs_file = os.path.join(subject_output_dir, \"epochs.fif\")\n",
    "        epochs.save(epochs_file, overwrite=True)\n",
    "        print(f\"Saved epochs to {epochs_file}\")\n",
    "        \n",
    "        # 4. Extract features\n",
    "        features_df = extract_features(epochs)\n",
    "        \n",
    "        # 5. Validate features\n",
    "        features_df = validate_features(features_df)\n",
    "        \n",
    "        # Save features\n",
    "        features_dir = os.path.join(subject_output_dir, \"features\")\n",
    "        os.makedirs(features_dir, exist_ok=True)\n",
    "        \n",
    "        features_file = os.path.join(features_dir, \"features.csv\")\n",
    "        features_df.to_csv(features_file, index=False)\n",
    "        print(f\"Saved features to {features_file}\")\n",
    "        \n",
    "        # 6. Plot feature distributions\n",
    "        plot_path = os.path.join(features_dir, \"feature_distributions.png\")\n",
    "        plot_feature_distributions(features_df, plot_path)\n",
    "        \n",
    "        return epochs, features_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing subject {subject}, session {session}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Processing {len(subjects)} subjects, {len(sessions)} sessions\")\n",
    "\n",
    "# Create main output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each subject and session\n",
    "results = {}\n",
    "\n",
    "for subject in subjects:\n",
    "    results[subject] = {}\n",
    "    \n",
    "    for session in sessions:\n",
    "        print(f\"\\nProcessing subject {subject}, session {session}\")\n",
    "        \n",
    "        epochs, features_df = process_subject_session(subject, session)\n",
    "        \n",
    "        if epochs is not None and features_df is not None:\n",
    "            results[subject][session] = {\n",
    "                \"epochs_count\": len(epochs),\n",
    "                \"features_count\": len(features_df),\n",
    "                \"class_counts\": features_df['class'].value_counts().to_dict()\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_file = os.path.join(output_dir, \"processing_summary.txt\")\n",
    "with open(summary_file, 'w') as f:\n",
    "    f.write(\"COG-BCI Processing Summary\\n\")\n",
    "    f.write(\"========================\\n\\n\")\n",
    "    \n",
    "    for subject, sessions_data in results.items():\n",
    "        f.write(f\"Subject {subject}:\\n\")\n",
    "        \n",
    "        for session, data in sessions_data.items():\n",
    "            f.write(f\"  Session {session}:\\n\")\n",
    "            f.write(f\"    Epochs: {data['epochs_count']}\\n\")\n",
    "            f.write(f\"    Features: {data['features_count']}\\n\")\n",
    "            f.write(f\"    Class distribution: {data['class_counts']}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Processing complete. Summary saved to {summary_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer model for EEG classification\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Multi-head self-attention (seq_len, batch, features)\n",
    "        x_tmp = x.permute(1, 0, 2)  # (seq_len, batch, features)\n",
    "        attn_out, _ = self.attention(x_tmp, x_tmp, x_tmp)\n",
    "        attn_out = attn_out.permute(1, 0, 2)  # (batch, seq_len, features)\n",
    "        \n",
    "        # Add & Norm\n",
    "        out1 = self.norm1(x + self.dropout(attn_out))\n",
    "        \n",
    "        # Feed Forward\n",
    "        ff_out = self.ff(out1)\n",
    "        \n",
    "        # Add & Norm\n",
    "        out2 = self.norm2(out1 + ff_out)\n",
    "        \n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, seq_len=10, embed_dim=64, \n",
    "                 num_heads=4, ff_dim=128, num_transformer_blocks=2, dropout=0.2):\n",
    "        super(EEGTransformer, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        if input_dim < seq_len:\n",
    "            self.features_per_seq = 1\n",
    "            self.seq_len = input_dim\n",
    "        else:\n",
    "            self.features_per_seq = input_dim // seq_len\n",
    "            if input_dim % seq_len != 0:\n",
    "                self.features_per_seq += 1\n",
    "                \n",
    "        print(f\"Creating model with input_dim={input_dim}, seq_len={self.seq_len}, \"\n",
    "              f\"features_per_seq={self.features_per_seq}, embed_dim={embed_dim}\")\n",
    "        \n",
    "        self.input_projection = nn.Linear(self.features_per_seq, embed_dim)\n",
    "        \n",
    "        # Positional embedding\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.seq_len, embed_dim))\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_dim, num_heads, ff_dim, dropout)\n",
    "            for _ in range(num_transformer_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        if x.shape[1] != self.input_dim:\n",
    "            print(f\"Warning: Input dimension mismatch. Expected {self.input_dim}, got {x.shape[1]}\")\n",
    "            self.input_dim = x.shape[1]\n",
    "            \n",
    "            if self.input_dim < self.seq_len:\n",
    "                self.seq_len = self.input_dim\n",
    "                self.features_per_seq = 1\n",
    "            else:\n",
    "                self.features_per_seq = self.input_dim // self.seq_len\n",
    "                if self.input_dim % self.seq_len != 0:\n",
    "                    self.features_per_seq += 1\n",
    "            \n",
    "            new_projection = nn.Linear(self.features_per_seq, self.embed_dim).to(x.device)\n",
    "            if hasattr(self, 'input_projection'):\n",
    "                if self.input_projection.weight.shape[1] == new_projection.weight.shape[1]:\n",
    "                    with torch.no_grad():\n",
    "                        new_projection.weight.copy_(self.input_projection.weight)\n",
    "                        new_projection.bias.copy_(self.input_projection.bias)\n",
    "            self.input_projection = new_projection\n",
    "            \n",
    "            if hasattr(self, 'pos_embedding') and self.pos_embedding.shape[1] != self.seq_len:\n",
    "                new_pos_embedding = nn.Parameter(torch.randn(1, self.seq_len, self.embed_dim).to(x.device))\n",
    "                if self.pos_embedding.shape[1] < self.seq_len:\n",
    "                    with torch.no_grad():\n",
    "                        new_pos_embedding[0, :self.pos_embedding.shape[1], :] = self.pos_embedding\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        new_pos_embedding = nn.Parameter(self.pos_embedding[:, :self.seq_len, :])\n",
    "                self.pos_embedding = new_pos_embedding\n",
    "        \n",
    "        if self.seq_len == 1:\n",
    "            x = x.unsqueeze(1)  # (batch, 1, features)\n",
    "        else:\n",
    "            if x.shape[1] < self.seq_len * self.features_per_seq:\n",
    "                padding = self.seq_len * self.features_per_seq - x.shape[1]\n",
    "                x = torch.cat([x, torch.zeros(batch_size, padding, device=x.device)], dim=1)\n",
    "            x = x.reshape(batch_size, self.seq_len, self.features_per_seq)\n",
    "        \n",
    "        x = self.input_projection(x)  # (batch, seq_len, embed_dim)\n",
    "        \n",
    "        # Add positional embeddings\n",
    "        x = x + self.pos_embedding\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Global pooling across sequence dimension\n",
    "        x = x.permute(0, 2, 1)  # (batch, embed_dim, seq_len)\n",
    "        x = self.global_pool(x).squeeze(-1)  # (batch, embed_dim)\n",
    "        \n",
    "        # Classification\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for EEG features\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y=None, transform=None):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y) if y is not None else None\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        else:\n",
    "            return self.X[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(feature_dir, subjects=None):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_subjects = []\n",
    "    all_sessions = []\n",
    "    feature_files = []\n",
    "    \n",
    "    # Find all feature CSV files\n",
    "    if subjects is None:\n",
    "        feature_files = glob.glob(f\"{feature_dir}/sub-*/ses-*/features/features.csv\")\n",
    "    else:\n",
    "        for subject in subjects:\n",
    "            subj_files = glob.glob(f\"{feature_dir}/sub-{subject}/ses-*/features/features.csv\")\n",
    "            feature_files.extend(subj_files)\n",
    "    \n",
    "    print(f\"Found {len(feature_files)} feature files\")\n",
    "    \n",
    "    feature_dimensions = []\n",
    "    \n",
    "    for file_path in feature_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            if 'class' not in df.columns or df.shape[0] == 0:\n",
    "                continue\n",
    "                \n",
    "            n_features = df.shape[1] - 2\n",
    "            feature_dimensions.append(n_features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error scanning {file_path}: {str(e)}\")\n",
    "    \n",
    "    if not feature_dimensions:\n",
    "        print(\"No valid feature files found\")\n",
    "        return None\n",
    "        \n",
    "    feature_dim_counts = pd.Series(feature_dimensions).value_counts()\n",
    "    most_common_dim = feature_dim_counts.index[0]\n",
    "    \n",
    "    print(f\"Feature dimensions across files: {dict(feature_dim_counts)}\")\n",
    "    print(f\"Using most common dimension: {most_common_dim} features\")\n",
    "    \n",
    "    for file_path in feature_files:\n",
    "        try:\n",
    "            # Extract subject and session from path\n",
    "            path_parts = file_path.split(os.sep)\n",
    "            subject_idx = [i for i, p in enumerate(path_parts) if p.startswith('sub-')][0]\n",
    "            session_idx = [i for i, p in enumerate(path_parts) if p.startswith('ses-')][0]\n",
    "            \n",
    "            subject = path_parts[subject_idx].split('-')[1]\n",
    "            session = path_parts[session_idx].split('-')[1]\n",
    "            \n",
    "            # Load features\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            if 'class' not in df.columns or df.shape[0] == 0:\n",
    "                print(f\"Skipping {file_path}: No class column or empty data\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"Loading {file_path}: {df.shape[0]} samples, classes: {df['class'].unique()}\")\n",
    "            \n",
    "            # Extract features and labels\n",
    "            X = df.drop(['label', 'class'], axis=1).values\n",
    "            y = df['class'].values\n",
    "            \n",
    "            # Check if feature dimension matches the most common dimension\n",
    "            if X.shape[1] != most_common_dim:\n",
    "                print(f\"  Warning: Feature dimension mismatch in {file_path}: {X.shape[1]} vs {most_common_dim}\")\n",
    "                \n",
    "                if X.shape[1] > most_common_dim:\n",
    "                    # Too many features - select first most_common_dim features\n",
    "                    print(f\"  Selecting first {most_common_dim} features\")\n",
    "                    X = X[:, :most_common_dim]\n",
    "                else:\n",
    "                    # Too few features - pad with zeros\n",
    "                    print(f\"  Padding with {most_common_dim - X.shape[1]} zeros\")\n",
    "                    padding = np.zeros((X.shape[0], most_common_dim - X.shape[1]))\n",
    "                    X = np.hstack((X, padding))\n",
    "            \n",
    "            # Store data\n",
    "            all_features.append(X)\n",
    "            all_labels.append(y)\n",
    "            all_subjects.extend([subject] * len(y))\n",
    "            all_sessions.extend([session] * len(y))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {str(e)}\")\n",
    "    \n",
    "    # Convert to arrays\n",
    "    if all_features:\n",
    "        features_data = {\n",
    "            'X': np.vstack(all_features),\n",
    "            'y': np.concatenate(all_labels),\n",
    "            'subjects': np.array(all_subjects),\n",
    "            'sessions': np.array(all_sessions)\n",
    "        }\n",
    "        \n",
    "        print(f\"Loaded {features_data['X'].shape[0]} samples with {features_data['X'].shape[1]} features\")\n",
    "        \n",
    "        return features_data\n",
    "    else:\n",
    "        print(\"No valid features found\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_model(features_data):\n",
    "    # Extract features and labels\n",
    "    X = features_data['X']\n",
    "    y_str = features_data['y']\n",
    "    subjects = features_data['subjects']\n",
    "    \n",
    "    # Standardize features\n",
    "    print(\"Standardizing features\")\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Encode string labels to integers\n",
    "    print(\"Encoding labels\")\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y_str)\n",
    "    \n",
    "    print(f\"Classes: {label_encoder.classes_}\")\n",
    "    \n",
    "    return X_scaled, y, subjects, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, \n",
    "               num_epochs=100, device='cuda', patience=15, verbose=True):\n",
    "    \n",
    "    # Initialize history dictionary\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    # Initialize early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    no_improve = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        if verbose:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "            print('-' * 10)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update statistics\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        epoch_train_loss = train_loss / train_total\n",
    "        epoch_train_acc = train_correct / train_total\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['train_acc'].append(epoch_train_acc)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Update statistics\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        epoch_val_loss = val_loss / val_total\n",
    "        epoch_val_acc = val_correct / val_total\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['val_acc'].append(epoch_val_acc)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Train loss: {epoch_train_loss:.4f}, acc: {epoch_train_acc:.4f}')\n",
    "            print(f'Val loss: {epoch_val_loss:.4f}, acc: {epoch_val_acc:.4f}')\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(epoch_val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                if verbose:\n",
    "                    print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "    \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # Collect results\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_names, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Determine format based on whether the matrix is normalized\n",
    "    fmt = '.2f' if np.any(np.issubdtype(cm.dtype, np.floating)) else 'd'\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt=fmt, cmap=cmap, xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train')\n",
    "    plt.plot(history['val_loss'], label='Validation')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train')\n",
    "    plt.plot(history['val_acc'], label='Validation')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_subject_transfer(X, y, subjects, label_encoder, output_dir, \n",
    "                               device='cuda', batch_size=32, num_epochs=100, patience=15):\n",
    "    # Initialize leave-one-subject-out cross-validation\n",
    "    logo = LeaveOneGroupOut()\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    results = {\n",
    "        'accuracy': [],\n",
    "        'f1_score': [],\n",
    "        'subject': [],\n",
    "        'confusion_matrix': [],\n",
    "        'history': []\n",
    "    }\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create log file\n",
    "    log_file = os.path.join(output_dir, 'training_log.txt')\n",
    "    with open(log_file, 'w') as f:\n",
    "        f.write(f\"Training started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Features shape: {X.shape}\\n\")\n",
    "        f.write(f\"Classes: {label_encoder.classes_}\\n\")\n",
    "        f.write(f\"Subjects: {np.unique(subjects)}\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\\n\")\n",
    "    \n",
    "    # Find unique subject IDs to iterate over, rather than using LOGO\n",
    "    # This ensures we explicitly control which subject is held out in each fold\n",
    "    unique_subjects = np.unique(subjects)\n",
    "    \n",
    "    # Run cross-validation\n",
    "    for fold, test_subject in enumerate(unique_subjects):\n",
    "        print(f\"\\n=== Fold {fold+1}: Testing on subject {test_subject} ===\")\n",
    "        \n",
    "        # Manually create train/test split\n",
    "        test_idx = np.where(subjects == test_subject)[0]\n",
    "        train_idx = np.where(subjects != test_subject)[0]\n",
    "        \n",
    "        # Add to log\n",
    "        with open(log_file, 'a') as f:\n",
    "            f.write(f\"Fold {fold+1}: Testing on subject {test_subject}\\n\")\n",
    "        \n",
    "        # Create fold directory\n",
    "        fold_dir = os.path.join(output_dir, f\"fold_{fold+1}_subject_{test_subject}\")\n",
    "        os.makedirs(fold_dir, exist_ok=True)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # Check if we have enough samples\n",
    "        if len(X_train) < 10 or len(X_test) < 5:\n",
    "            print(f\"Warning: Not enough samples for subject {test_subject}. Skipping.\")\n",
    "            with open(log_file, 'a') as f:\n",
    "                f.write(f\"  Warning: Not enough samples for subject {test_subject}. Skipping.\\n\\n\")\n",
    "            continue\n",
    "            \n",
    "        # Log input dimensions\n",
    "        with open(log_file, 'a') as f:\n",
    "            f.write(f\"  Input dimensions - X_train: {X_train.shape}, X_test: {X_test.shape}\\n\")\n",
    "        \n",
    "        # Split training data into train and validation\n",
    "        np.random.seed(42)\n",
    "        val_size = min(int(0.2 * len(X_train)), 50)  # Cap validation size\n",
    "        val_idx = np.random.choice(len(X_train), val_size, replace=False)\n",
    "        train_mask = np.ones(len(X_train), dtype=bool)\n",
    "        train_mask[val_idx] = False\n",
    "        \n",
    "        X_val = X_train[val_idx]\n",
    "        y_val = y_train[val_idx]\n",
    "        X_train_final = X_train[train_mask]\n",
    "        y_train_final = y_train[train_mask]\n",
    "        \n",
    "        # Log dataset splits\n",
    "        with open(log_file, 'a') as f:\n",
    "            f.write(f\"  Training samples: {len(X_train_final)}\\n\")\n",
    "            f.write(f\"  Validation samples: {len(X_val)}\\n\")\n",
    "            f.write(f\"  Test samples: {len(X_test)}\\n\")\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_dataset = EEGDataset(X_train_final, y_train_final)\n",
    "        val_dataset = EEGDataset(X_val, y_val)\n",
    "        test_dataset = EEGDataset(X_test, y_test)\n",
    "        \n",
    "        # Adjust batch size if needed\n",
    "        actual_batch_size = min(batch_size, len(X_train_final), len(X_val), len(X_test))\n",
    "        if actual_batch_size < batch_size:\n",
    "            print(f\"Warning: Reducing batch size to {actual_batch_size} due to small dataset\")\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=actual_batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=actual_batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=actual_batch_size, shuffle=False)\n",
    "        \n",
    "        # Create model\n",
    "        num_classes = len(np.unique(y))\n",
    "        input_dim = X_train.shape[1]\n",
    "        \n",
    "        # Define sequence length based on input dimension\n",
    "        seq_len = min(10, max(1, input_dim // 64))\n",
    "        \n",
    "        model = EEGTransformer(\n",
    "            input_dim=input_dim, \n",
    "            num_classes=num_classes,\n",
    "            seq_len=seq_len,\n",
    "            embed_dim=64,\n",
    "            num_heads=4,\n",
    "            ff_dim=128,\n",
    "            num_transformer_blocks=2,\n",
    "            dropout=0.2\n",
    "        ).to(device)\n",
    "        \n",
    "        # Log model details\n",
    "        with open(log_file, 'a') as f:\n",
    "            f.write(f\"  Model input_dim: {input_dim}\\n\")\n",
    "            f.write(f\"  Model seq_len: {seq_len}\\n\")\n",
    "            f.write(f\"  Model num_classes: {num_classes}\\n\")\n",
    "        \n",
    "        # Set up loss function, optimizer and scheduler\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-5, verbose=True\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Train model\n",
    "            model, history = train_model(\n",
    "                model=model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                criterion=criterion,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler,\n",
    "                num_epochs=num_epochs,\n",
    "                device=device,\n",
    "                patience=patience,\n",
    "                verbose=True\n",
    "            )\n",
    "            \n",
    "            # Save model\n",
    "            model_path = os.path.join(fold_dir, \"model.pt\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            \n",
    "            # Plot and save learning curves\n",
    "            fig = plot_learning_curves(history)\n",
    "            fig.savefig(os.path.join(fold_dir, \"learning_curves.png\"))\n",
    "            plt.close(fig)\n",
    "            \n",
    "            # Save training history\n",
    "            with open(os.path.join(fold_dir, \"history.json\"), 'w') as f:\n",
    "                # Convert numpy arrays to lists\n",
    "                history_json = {k: [float(val) for val in v] for k, v in history.items()}\n",
    "                json.dump(history_json, f, indent=4)\n",
    "            \n",
    "            # Evaluate model\n",
    "            metrics = evaluate_model(model, test_loader, device)\n",
    "            \n",
    "            # Log metrics\n",
    "            with open(log_file, 'a') as f:\n",
    "                f.write(f\"  Test accuracy: {metrics['accuracy']:.4f}\\n\")\n",
    "                f.write(f\"  Test F1 score: {metrics['f1_score']:.4f}\\n\\n\")\n",
    "            \n",
    "            # Plot and save confusion matrix\n",
    "            class_names = label_encoder.classes_\n",
    "            fig = plot_confusion_matrix(metrics['confusion_matrix'], class_names)\n",
    "            fig.savefig(os.path.join(fold_dir, \"confusion_matrix.png\"))\n",
    "            plt.close(fig)\n",
    "            \n",
    "            # Save metrics\n",
    "            with open(os.path.join(fold_dir, \"metrics.json\"), 'w') as f:\n",
    "                metrics_json = {\n",
    "                    'accuracy': float(metrics['accuracy']),\n",
    "                    'f1_score': float(metrics['f1_score']),\n",
    "                    'confusion_matrix': metrics['confusion_matrix'].tolist(),\n",
    "                    'class_names': class_names.tolist()\n",
    "                }\n",
    "                json.dump(metrics_json, f, indent=4)\n",
    "            \n",
    "            # Store results\n",
    "            results['accuracy'].append(metrics['accuracy'])\n",
    "            results['f1_score'].append(metrics['f1_score'])\n",
    "            results['subject'].append(test_subject)\n",
    "            results['confusion_matrix'].append(metrics['confusion_matrix'])\n",
    "            results['history'].append(history)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in fold {fold+1} (subject {test_subject}): {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            with open(log_file, 'a') as f:\n",
    "                f.write(f\"  Error: {str(e)}\\n\\n\")\n",
    "            \n",
    "            # Continue with next fold\n",
    "            continue\n",
    "    \n",
    "    # Check if we have any results\n",
    "    if len(results['accuracy']) == 0:\n",
    "        print(\"No successful folds completed. Check the logs for errors.\")\n",
    "        return results\n",
    "    \n",
    "    # Calculate average results\n",
    "    mean_accuracy = np.mean(results['accuracy'])\n",
    "    std_accuracy = np.std(results['accuracy'])\n",
    "    mean_f1 = np.mean(results['f1_score'])\n",
    "    std_f1 = np.std(results['f1_score'])\n",
    "    \n",
    "    # Log overall results\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f\"\\n=== Overall Results ===\\n\")\n",
    "        f.write(f\"Mean accuracy: {mean_accuracy:.4f} ± {std_accuracy:.4f}\\n\")\n",
    "        f.write(f\"Mean F1 score: {mean_f1:.4f} ± {std_f1:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Per-subject results:\\n\")\n",
    "        for i, subj in enumerate(results['subject']):\n",
    "            f.write(f\"  Subject {subj}: \"\n",
    "                   f\"Accuracy={results['accuracy'][i]:.4f}, \"\n",
    "                   f\"F1={results['f1_score'][i]:.4f}\\n\")\n",
    "    \n",
    "    # Save overall results\n",
    "    with open(os.path.join(output_dir, \"overall_results.json\"), 'w') as f:\n",
    "        overall_results = {\n",
    "            'mean_accuracy': float(mean_accuracy),\n",
    "            'std_accuracy': float(std_accuracy),\n",
    "            'mean_f1': float(mean_f1),\n",
    "            'std_f1': float(std_f1),\n",
    "            'per_subject': [\n",
    "                {\n",
    "                    'subject': str(subj),\n",
    "                    'accuracy': float(acc),\n",
    "                    'f1_score': float(f1)\n",
    "                }\n",
    "                for subj, acc, f1 in zip(results['subject'], results['accuracy'], results['f1_score'])\n",
    "            ]\n",
    "        }\n",
    "        json.dump(overall_results, f, indent=4)\n",
    "    \n",
    "    # Plot and save overall results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Bar plot of accuracy by subject\n",
    "    plt.bar(range(len(results['subject'])), results['accuracy'])\n",
    "    plt.axhline(y=mean_accuracy, color='r', linestyle='--', label=f'Mean accuracy: {mean_accuracy:.4f}')\n",
    "    \n",
    "    plt.title('Accuracy by Subject (Leave-One-Subject-Out CV)')\n",
    "    plt.xlabel('Subject')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(range(len(results['subject'])), results['subject'])\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir, \"overall_accuracy.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    avg_conf_matrix = np.zeros_like(results['confusion_matrix'][0], dtype=float)\n",
    "    for cm in results['confusion_matrix']:\n",
    "        # Normalize each confusion matrix by row (true label)\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "        if np.all(row_sums > 0):  # Avoid division by zero\n",
    "            cm_norm = cm / row_sums\n",
    "            avg_conf_matrix += cm_norm\n",
    "    \n",
    "    avg_conf_matrix /= len(results['confusion_matrix'])\n",
    "    \n",
    "    \n",
    "    fig = plot_confusion_matrix(avg_conf_matrix, class_names, title='Average Normalized Confusion Matrix')\n",
    "    fig.savefig(os.path.join(output_dir, \"average_confusion_matrix.png\"))\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print(f\"\\n=== Overall Results ===\")\n",
    "    print(f\"Mean accuracy: {mean_accuracy:.4f} ± {std_accuracy:.4f}\")\n",
    "    print(f\"Mean F1 score: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(feature_dir, output_dir, subjects=None):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    print(\"Loading features\")\n",
    "    features_data = load_features(feature_dir, subjects)\n",
    "    \n",
    "    if features_data is None or len(features_data['X']) == 0:\n",
    "        print(\"No features found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    print(\"Preparing data for model\")\n",
    "    X, y, subjects, label_encoder = prepare_data_for_model(features_data)\n",
    "    \n",
    "    \n",
    "    with open(os.path.join(output_dir, \"label_encoder.json\"), 'w') as f:\n",
    "        json.dump({\n",
    "            'classes': label_encoder.classes_.tolist(),\n",
    "            'transform': label_encoder.transform(label_encoder.classes_).tolist()\n",
    "        }, f, indent=4)\n",
    "    \n",
    "    \n",
    "    print(\"Training with subject transfer\")\n",
    "    results = train_with_subject_transfer(\n",
    "        X=X,\n",
    "        y=y,\n",
    "        subjects=subjects,\n",
    "        label_encoder=label_encoder,\n",
    "        output_dir=output_dir,\n",
    "        device=device,\n",
    "        batch_size=32,\n",
    "        num_epochs=100,\n",
    "        patience=15\n",
    "    )\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    print(f\"Results saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dir = \"processed_data\"\n",
    "output_dir = \"transformer_results\"\n",
    "subjects = None  # Process all subjects\n",
    "\n",
    "\n",
    "main(feature_dir, output_dir, subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BCI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
